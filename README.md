<p align="center">
  <img src="banner_airflow_anime_etl.png" alt="Airflow Anime ETL â€” Episode 1" width="100%">
</p>

# ğŸ¬ Airflow Anime ETL â€” A Jornada de um Pipeline

> Uma sÃ©rie de aprendizado e construÃ§Ã£o prÃ¡tica, parte por parte, de um pipeline de dados real com **Apache Airflow**, **Celery**, **Redis**, **PostgreSQL** e **Docker** â€” tendo como tema o universo dos **animes**! ğŸ¥âš¡

---

## ğŸ§­ IntroduÃ§Ã£o

Este projeto Ã© mais do que apenas cÃ³digo: Ã© uma **jornada**.

A ideia Ã© documentar o passo a passo da criaÃ§Ã£o de um **pipeline ETL completo**, mostrando como um Engenheiro de Dados projeta, estrutura e executa cada parte de um ecossistema moderno de orquestraÃ§Ã£o.

Cada parte representa uma etapa concreta do progresso â€” do ambiente inicial atÃ© a automaÃ§Ã£o total com DAGs distribuÃ­das.

---

## ğŸ“– SÃ©rie â€” A EvoluÃ§Ã£o do Projeto

| Parte | TÃ­tulo | Status | DescriÃ§Ã£o |
|:------:|:--------|:--------:|:-----------|
| ğŸ§© **Parte 1** | *O Nascimento do Ambiente* | âœ… ConcluÃ­da | Montagem do ecossistema Airflow com CeleryExecutor, Redis e PostgreSQL via Docker Compose. |
| âš™ï¸ **Parte 2** | *Primeira DAG: Extraindo o Poder dos Animes* | ğŸš§ Em desenvolvimento | CriaÃ§Ã£o da DAG ETL para extrair dados da **Jikan API**, transformÃ¡-los em CSV e armazenÃ¡-los no PostgreSQL. |
| ğŸ“Š **Parte 3** | *Da Pipeline ao Insight* | ğŸ”œ Em breve | ConexÃ£o com dashboards e monitoramento dos dados processados (Streamlit / Grafana). |

---

## ğŸ§© Parte 1 â€” O Nascimento do Ambiente

Nesta primeira fase, o objetivo foi **erguer as fundaÃ§Ãµes**: construir o ambiente completo do Airflow rodando com **CeleryExecutor** â€” o coraÃ§Ã£o que permite o paralelismo e a escalabilidade das tasks.

### ğŸ”§ Etapas realizadas

- CriaÃ§Ã£o do diretÃ³rio `airflow_anime`
- ConfiguraÃ§Ã£o do `docker-compose.yml` com os serviÃ§os:
  - `webserver`, `scheduler`, `worker`, `flower`
  - `postgres` (banco de metadados)
  - `redis` (message broker do Celery)
- InicializaÃ§Ã£o do banco (`airflow db init`)
- CriaÃ§Ã£o do usuÃ¡rio `admin`
- CorreÃ§Ã£o de permissÃµes e volumes
- Painel Airflow disponÃ­vel em: [http://localhost:8080](http://localhost:8080)
- Painel Flower disponÃ­vel em: [http://localhost:5555](http://localhost:5555)

ğŸ§  *Resultado:* Ambiente 100% funcional, pronto para receber as DAGs da Parte 2.

---

## âš™ï¸ Parte 2 â€” Primeira DAG: Extraindo o Poder dos Animes

A segunda fase serÃ¡ o verdadeiro **primeiro voo do pipeline**.  
O objetivo Ã© criar uma **DAG diÃ¡ria** que:

1. **Extraia** informaÃ§Ãµes da [Jikan API](https://docs.api.jikan.moe/)  
   (tÃ­tulo, episÃ³dios, nota, popularidade, data de lanÃ§amento);
2. **Transforme** os dados com `pandas`;
3. **Carregue** os resultados:
   - em `.csv` (camada raw e processed);
   - no banco **PostgreSQL** dentro do container.
4. Tudo isso, **distribuÃ­do via Celery**, com logs e monitoramento no Flower.

ğŸ“¦ *Stack usada:* `requests`, `pandas`, `PostgresHook`, `psycopg2`.

ğŸ” *Agendamento:* diÃ¡rio (`@daily`)

---

## ğŸ“Š Parte 3 â€” Da Pipeline ao Insight

A terceira parte trarÃ¡ o lado analÃ­tico da jornada:
- ConexÃ£o do banco ao **Streamlit** para criar dashboards interativos;
- VisualizaÃ§Ã£o da evoluÃ§Ã£o de notas, popularidade e tendÃªncias de animes;
- (Possivelmente) alertas automÃ¡ticos via Airflow.

ğŸ¯ *Objetivo:* Fechar o ciclo do dado â€” da extraÃ§Ã£o ao insight.

---

## âš™ï¸ Stack tÃ©cnica geral

| Componente | FunÃ§Ã£o |
|-------------|--------|
| **Apache Airflow** | OrquestraÃ§Ã£o das DAGs |
| **Celery** | ExecuÃ§Ã£o distribuÃ­da das tasks |
| **Redis** | Message Broker |
| **PostgreSQL** | Armazenamento e persistÃªncia dos dados |
| **Docker Compose** | Infraestrutura de containers |
| **Python (requests, pandas)** | ExtraÃ§Ã£o e transformaÃ§Ã£o dos dados |
| **Jikan API** | Fonte dos dados de anime (REST pÃºblica) |

---

## ğŸ§  Sobre o projeto

Criado por **Takeshy Takatsu (LilKeshy)** como parte do aprendizado em **Engenharia de Dados**, unindo:
- OrquestraÃ§Ã£o (Airflow)  
- Paralelismo (Celery)  
- APIs reais (Jikan)  
- AutomaÃ§Ã£o e monitoramento (Flower e Postgres)

ğŸ“… *Linha do tempo:*
- **Out/2025:** Parte 1 concluÃ­da âœ…  
- **Outâ€“Nov/2025:** Desenvolvimento da Parte 2  
- **Nov/2025+:** Parte 3 (visualizaÃ§Ã£o e deploy final)

---

## ğŸª„ Como rodar localmente

```bash
# Subir containers
docker compose up -d

# Inicializar Airflow (caso seja a primeira vez)
docker compose run airflow-init

# Acessar painÃ©is
http://localhost:8080   # Airflow
http://localhost:5555   # Flower
